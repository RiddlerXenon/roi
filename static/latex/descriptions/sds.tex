\documentclass{article}

% Language setting
\usepackage[russian]{babel}

% Set page size and margins
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{amssymb,mathtools,bm,dsfont}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\newcommand{\1}{\mathds{1}}

\begin{document}

Алгоритм стохастического диффузионного поиска формализуется как популяционная метаэвристика на основе коммуникационной модели с механизмом диффузии информации между агентами в дискретном времени [1, 2]. Каждый агент $i = 1, \ldots, N$ на итерации $t \in \mathbb{N}$ характеризуется состоянием $(h_i^{(t)}, s_i^{(t)}) \in \mathcal{S} \times \{0,1\}$, где $h_i^{(t)}$ — текущая гипотеза в пространстве поиска $\mathcal{S}$, а $s_i^{(t)}$ — булев индикатор активности агента. Управляющая динамика определяется двухфазным итерационным процессом с стохастической функцией частичной оценки $\phi: \mathcal{S} \times \Omega \rightarrow \{0,1\}$ и адаптивным механизмом диффузии информации между активными и неактивными агентами [1, 3].

Пространство поиска задается как $\mathcal{S} = [-R, R]^2 \subset \mathbb{R}^2$ с радиусом области $R > 0$. Целевая функция $f: \mathcal{S} \rightarrow \mathbb{R}_+$ подлежит максимизации. Множество тестовых компонент $\Omega$ представляет собой равномерное распределение на $\mathcal{S}$, что обеспечивает стохастическую природу оценки без необходимости глобальной нормализации функции приспособленности [2, 4].

Функция частичной оценки реализуется как стохастическое сравнение:
\begin{equation}
    \phi _i^{(t)} = \mathbb{1} \{ f(h_i^{(t)}) \geq f(\omega^{(t)}) \}
\end{equation}
где $\omega^{(t)} \sim \mathcal{U}(\mathcal{S})$ — случайная точка сравнения, генерируемая независимо для каждого агента на каждой итерации. Статус активности определяется непосредственно результатом тестирования:
\begin{equation}
    s_i^{(t)} = \phi _i^{(t)}
\end{equation}

Данный подход гарантирует, что агенты с гипотезами высокого качества имеют большую вероятность стать активными, при этом сохраняя стохастическую устойчивость алгоритма [1, 2].

Алгоритм состоит из двух основных фаз, выполняемых последовательно на каждой итерации: \textit{фазы тестирования} и \textit{фазы диффузии}. В фазе тестирования для каждого агента $i$ вычисляется новый статус активности согласно уравнению (1) с использованием текущей гипотезы $h_i^{(t)}$ и случайно выбранной тестовой компоненты $\omega^{(t)}$. Это позволяет распределенно оценить относительное качество гипотез в популяции без централизованного ранжирования [3, 4].

Фаза диффузии реализует адаптивный механизм обмена информацией с поддержкой мультимодальности. Множество активных агентов на итерации $t$ определяется как $\mathcal{W}^{(t)} = \{i : s_i^{(t)} = 1\}$. Правило обновления гипотез формализуется следующим образом:

При $|\mathcal{W}^{(t)}| = 0$ (отсутствие активных агентов) выполняется адаптивный перезапуск:
\begin{equation}
    h_i^{(t+1)} = \begin{cases}
        U(\mathcal{S}), & \text{с вероятностью } p_{\text{restart}} \\
        h_i^{(t)}, & \text{иначе}
    \end{cases}
\end{equation}
где $p_{\text{restart}} \in [0,1]$ — параметр интенсивности перезапуска, $U(\mathcal{S})$ — равномерное распределение на пространстве поиска.

При $|\mathcal{W}^{(t)}| > 0$ осуществляется стандартная диффузия от активных агентов:
\begin{equation}
    h_i^{(t+1)} = \begin{cases}
        h_i^{(t)}, & \text{если } s_i^{(t)} = 1 \\
        h_j^{(t)}, & \text{если } s_i^{(t)} = 0, \text{ где } j \sim \mathcal{U}(\mathcal{W}^{(t)})
    \end{cases}
\end{equation}

Данный механизм обеспечивает диффузию информации о высококачественных решениях через популяцию, одновременно предотвращая полную стагнацию при временном отсутствии приемлемых гипотез [2, 3].

После диффузии все агенты подвергаются стохастической мутации для обеспечения разведки:
\begin{equation}
    h_i^{(t+1)} \gets \text{clip}_{\mathcal{S}}\left(h_i^{(t+1)} + \sigma^{(t)} \cdot \mathcal{N}(0, I_d)\right)
\end{equation}
где $\text{clip}_{\mathcal{S}}(\cdot)$ — оператор проекции на область $\mathcal{S}$, $\mathcal{N}(0, I_d)$ — многомерное нормальное распределение, $\sigma^{(t)}$ — адаптивная дисперсия шума:
\begin{equation}
    \sigma^{(t)} = \begin{cases}
        \sigma _0 \cdot \rho^t, & \text{при адаптивном затухании} \\
        \sigma _0, & \text{при постоянной интенсивности}
    \end{cases}
\end{equation}
с параметрами $\sigma_0 > 0$ (начальная дисперсия) и $\rho \in (0,1)$ (коэффициент затухания) [4].

Ключевым свойством алгоритма является формирование стационарного распределения популяции, пропорционального качеству решений. В равновесном состоянии ожидаемая концентрация агентов в окрестности точки $h \in \mathcal{S}$ определяется как:
\begin{equation}
    \pi(h) \propto \mathbb{P}\{f(h) \geq f(\Omega)\} = \int_{\mathcal{S}} \mathbb{1} \{f(h) \geq f(u)\} du
\end{equation}
где интегрирование ведется по равномерному распределению на $\mathcal{S}$ [1, 2].

Для мультимодальных функций алгоритм естественным образом поддерживает несколько кластеров агентов вокруг различных локальных максимумов. Размер кластера в окрестности локального максимума $h^* \in \mathcal{S}$ в стационарном режиме приближенно равен:
\begin{equation}
    N(h^*) \approx N \cdot \frac{\pi(h^*)}{\sum_{h \in \text{Modes}} \pi(h)}
\end{equation}
где $\text{Modes}$ — множество значимых локальных максимумов целевой функции [3, 4].

Сходимость алгоритма к глобальному оптимуму обеспечивается при выполнении условий эргодичности марковской цепи состояний популяции. Если глобальный максимум $h^*_{\text{global}}$ имеет строго большую вероятность успеха тестирования $\pi(h^*_{\text{global}}) > \pi(h)$ для всех $h \neq h^*_{\text{global}}$, то популяция асимптотически концентрируется в его окрестности с вероятностью единица [2, 3].

Вычислительная сложность одной итерации составляет $O(N)$, что обеспечивает масштабируемость алгоритма для больших популяций. Эффективность существенно зависит от выбора параметров $\sigma_0$, $p_{\text{restart}}$ и стратегии адаптации дисперсии шума, которые должны балансировать интенсивность разведки (exploration) и эксплуатации (exploitation) найденных решений [1, 4].

Конечный алгоритм формализуется следующим образом:

\begin{algorithm}[H]
\caption{Стохастический диффузионный поиск}
    \KwIn{Размер популяции $N \in \mathbb N$; пространство поиска $\mathcal S = [-R,R]^2$; целевая функция $f: \mathcal S \rightarrow \mathbb R _+$; параметры $\sigma _0 > 0$, $p_ {\text restart} \in [0,1]$, $\rho \in (0,1)$; максимальное число итераций $T$}
\KwOut{Лучшая найденная гипотеза $h^*$ и её качество $f^*$}

\textbf{Инициализация:} 
\For{$i = 1, 2, \ldots, N$}{
    $h_i^{(0)} \sim \mathcal{U}(\mathcal{S})$
}

\For{$t = 0, 1, \ldots, T-1$}{
    $\mathcal{W}^{(t)} \gets \emptyset$\;
    
    \tcp{Фаза тестирования}
    \For{$i = 1, 2, \ldots, N$}{
        Сгенерировать $\omega^{(t)} \sim \mathcal{U}(\mathcal{S})$\;
        $s_i^{(t)} \gets \mathbb{1} \{f(h_i^{(t)}) \geq f(\omega^{(t)})\}$\;
        \If{$s_i^{(t)} = 1$}{
            $\mathcal{W}^{(t)} \gets \mathcal{W}^{(t)} \cup \{i\}$\;
        }
    }
    
    \tcp{Фаза диффузии}
    \If{$|\mathcal{W}^{(t)}| = 0$}{
        \For{$i = 1, 2, \ldots, N$}{
            \If{$\xi \sim \mathcal{U}(0,1) \leq p_{\text{restart}}$}{
                $h_i^{(t+1)} \sim \mathcal{U}(\mathcal{S})$\;
            }
            \Else{
                $h_i^{(t+1)} \gets h_i^{(t)}$\;
            }
        }
    }
    \Else{
        \For{$i = 1, 2, \ldots, N$}{
            \If{$s_i^{(t)} = 1$}{
                $h_i^{(t+1)} \gets h_i^{(t)}$\;
            }
            \Else{
                Выбрать $j \sim \mathcal{U}(\mathcal{W}^{(t)})$\;
                $h_i^{(t+1)} \gets h_j^{(t)}$\;
            }
        }
    }
    
    \tcp{Фаза разведки}
    Вычислить $\sigma^{(t)}$ согласно уравнению (7)\;
    \For{$i = 1, 2, \ldots, N$}{
        $h_i^{(t+1)} \gets \text{clip}_{\mathcal{S}}\left(h_i^{(t+1)} + \sigma^{(t)} \cdot \mathcal{N}(0, I_2)\right)$\;
    }
}

$h^* \gets \arg\max_{i} f(h_i^{(T)})$, $f^* \gets f(h^*)$\;

\KwRet{$(h^*, f^*)$}
\end{algorithm}

Алгоритм стохастического диффузионного поиска представляет собой эффективный инструмент для решения задач глобальной мультимодальной оптимизации, сочетающий простоту реализации с теоретически обоснованными свойствами сходимости. Естественная поддержка параллелизации, минимальные требования к настройке параметров и способность к автоматическому обнаружению множественных оптимумов делают его привлекательной альтернативой традиционным метаэвристическим методам для широкого класса практических задач оптимизации в условиях неопределенности [1, 2, 4].

\noindent\hrulefill

1. Bishop, J.M. (1989). Stochastic searching networks. Proceedings of 1st IEE Conference on Artificial Neural Networks, London, UK, 329-331.

2. Nasuto, S.J., Bishop, J.M. (1999). Convergence analysis of stochastic diffusion search. Parallel Algorithms and Applications, 14(2), 89-107.

3. Al-Rifaie, M.M., Bishop, J.M. (2013). Stochastic diffusion search review. Paladyn, Journal of Behavioral Robotics, 4(3), 155-173.

4. Grech-Cini, H., McKee, G. (1993). Locating multiple optima using the stochastic diffusion search. Proceedings of the IEEE Conference on Evolutionary Computation, 259-264.

\end{document}
